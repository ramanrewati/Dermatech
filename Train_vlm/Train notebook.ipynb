{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'LLaMA-Factory'...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/hiyouga/LLaMA-Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/LLaMA-Factory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd LLaMA-Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/teamspace/studios/this_studio/LLaMA-Factory'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers<=4.45.2,>=4.41.2 (from -r requirements.txt (line 1))\n",
      "  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting datasets<=2.21.0,>=2.16.0 (from -r requirements.txt (line 2))\n",
      "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting accelerate<=0.34.2,>=0.30.1 (from -r requirements.txt (line 3))\n",
      "  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting peft<=0.12.0,>=0.11.1 (from -r requirements.txt (line 4))\n",
      "  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting trl<=0.9.6,>=0.8.6 (from -r requirements.txt (line 5))\n",
      "  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting gradio>=4.0.0 (from -r requirements.txt (line 6))\n",
      "  Downloading gradio-5.0.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (2.1.4)\n",
      "Requirement already satisfied: scipy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (1.11.4)\n",
      "Collecting einops (from -r requirements.txt (line 9))\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting sentencepiece (from -r requirements.txt (line 10))\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting tiktoken (from -r requirements.txt (line 11))\n",
      "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: protobuf in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (4.23.4)\n",
      "Requirement already satisfied: uvicorn in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (0.31.0)\n",
      "Requirement already satisfied: pydantic in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (2.9.2)\n",
      "Requirement already satisfied: fastapi in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from -r requirements.txt (line 15)) (0.115.0)\n",
      "Collecting sse-starlette (from -r requirements.txt (line 16))\n",
      "  Downloading sse_starlette-2.1.3-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from -r requirements.txt (line 17)) (3.8.2)\n",
      "Requirement already satisfied: fire in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from -r requirements.txt (line 18)) (0.7.0)\n",
      "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from -r requirements.txt (line 19)) (24.1)\n",
      "Requirement already satisfied: pyyaml in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from -r requirements.txt (line 20)) (6.0.2)\n",
      "Requirement already satisfied: numpy<2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from -r requirements.txt (line 21)) (1.26.4)\n",
      "Collecting av (from -r requirements.txt (line 22))\n",
      "  Downloading av-13.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers<=4.45.2,>=4.41.2->-r requirements.txt (line 1)) (3.16.1)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers<=4.45.2,>=4.41.2->-r requirements.txt (line 1))\n",
      "  Downloading huggingface_hub-0.25.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers<=4.45.2,>=4.41.2->-r requirements.txt (line 1))\n",
      "  Downloading regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers<=4.45.2,>=4.41.2->-r requirements.txt (line 1)) (2.32.3)\n",
      "Collecting safetensors>=0.4.1 (from transformers<=4.45.2,>=4.41.2->-r requirements.txt (line 1))\n",
      "  Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers<=4.45.2,>=4.41.2->-r requirements.txt (line 1))\n",
      "  Downloading tokenizers-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers<=4.45.2,>=4.41.2->-r requirements.txt (line 1)) (4.66.5)\n",
      "Collecting pyarrow>=15.0.0 (from datasets<=2.21.0,>=2.16.0->-r requirements.txt (line 2))\n",
      "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets<=2.21.0,>=2.16.0->-r requirements.txt (line 2))\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting xxhash (from datasets<=2.21.0,>=2.16.0->-r requirements.txt (line 2))\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets<=2.21.0,>=2.16.0->-r requirements.txt (line 2))\n",
      "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.6.1,>=2023.1.0 (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets<=2.21.0,>=2.16.0->-r requirements.txt (line 2))\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets<=2.21.0,>=2.16.0->-r requirements.txt (line 2)) (3.10.8)\n",
      "Requirement already satisfied: psutil in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate<=0.34.2,>=0.30.1->-r requirements.txt (line 3)) (6.0.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate<=0.34.2,>=0.30.1->-r requirements.txt (line 3)) (2.2.1+cu121)\n",
      "Collecting tyro>=0.5.11 (from trl<=0.9.6,>=0.8.6->-r requirements.txt (line 5))\n",
      "  Downloading tyro-0.8.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (4.6.0)\n",
      "Collecting ffmpy (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting gradio-client==1.4.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "  Downloading gradio_client-1.4.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (0.27.2)\n",
      "Requirement already satisfied: jinja2<4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (3.1.4)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (2.1.5)\n",
      "Collecting orjson~=3.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (10.4.0)\n",
      "Collecting pydub (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (0.0.12)\n",
      "Collecting ruff>=0.2.2 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "  Downloading ruff-0.6.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting tomlkit==0.12.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "  Downloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (4.12.2)\n",
      "Collecting websockets<13.0,>=10.0 (from gradio-client==1.4.0->gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas>=2.0.0->-r requirements.txt (line 7)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas>=2.0.0->-r requirements.txt (line 7)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas>=2.0.0->-r requirements.txt (line 7)) (2024.2)\n",
      "Requirement already satisfied: click>=7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from uvicorn->-r requirements.txt (line 13)) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from uvicorn->-r requirements.txt (line 13)) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pydantic->-r requirements.txt (line 14)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pydantic->-r requirements.txt (line 14)) (2.23.4)\n",
      "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fastapi->-r requirements.txt (line 15)) (0.38.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (3.1.4)\n",
      "Requirement already satisfied: termcolor in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fire->-r requirements.txt (line 18)) (2.4.0)\n",
      "Requirement already satisfied: idna>=2.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->-r requirements.txt (line 6)) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->-r requirements.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->-r requirements.txt (line 6)) (1.2.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->-r requirements.txt (line 2)) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->-r requirements.txt (line 2)) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->-r requirements.txt (line 2)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->-r requirements.txt (line 2)) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->-r requirements.txt (line 2)) (1.13.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->-r requirements.txt (line 2)) (4.0.3)\n",
      "Requirement already satisfied: certifi in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpx>=0.24.1->gradio>=4.0.0->-r requirements.txt (line 6)) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpx>=0.24.1->gradio>=4.0.0->-r requirements.txt (line 6)) (1.0.6)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->-r requirements.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers<=4.45.2,>=4.41.2->-r requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers<=4.45.2,>=4.41.2->-r requirements.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate<=0.34.2,>=0.30.1->-r requirements.txt (line 3)) (1.13.3)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate<=0.34.2,>=0.30.1->-r requirements.txt (line 3)) (3.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate<=0.34.2,>=0.30.1->-r requirements.txt (line 3)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate<=0.34.2,>=0.30.1->-r requirements.txt (line 3)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate<=0.34.2,>=0.30.1->-r requirements.txt (line 3)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate<=0.34.2,>=0.30.1->-r requirements.txt (line 3)) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate<=0.34.2,>=0.30.1->-r requirements.txt (line 3)) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate<=0.34.2,>=0.30.1->-r requirements.txt (line 3)) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate<=0.34.2,>=0.30.1->-r requirements.txt (line 3)) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate<=0.34.2,>=0.30.1->-r requirements.txt (line 3)) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate<=0.34.2,>=0.30.1->-r requirements.txt (line 3)) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate<=0.34.2,>=0.30.1->-r requirements.txt (line 3)) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate<=0.34.2,>=0.30.1->-r requirements.txt (line 3)) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate<=0.34.2,>=0.30.1->-r requirements.txt (line 3)) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate<=0.34.2,>=0.30.1->-r requirements.txt (line 3)) (12.6.77)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (13.9.1)\n",
      "Collecting docstring-parser>=0.16 (from tyro>=0.5.11->trl<=0.9.6,>=0.8.6->-r requirements.txt (line 5))\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl<=0.9.6,>=0.8.6->-r requirements.txt (line 5))\n",
      "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets<=2.21.0,>=2.16.0->-r requirements.txt (line 2))\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (2.18.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate<=0.34.2,>=0.30.1->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (0.1.2)\n",
      "Downloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m113.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n",
      "Downloading peft-0.12.0-py3-none-any.whl (296 kB)\n",
      "Downloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
      "Downloading gradio-5.0.0-py3-none-any.whl (42.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m156.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-1.4.0-py3-none-any.whl (319 kB)\n",
      "Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
      "Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sse_starlette-2.1.3-py3-none-any.whl (9.4 kB)\n",
      "Downloading av-13.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.1/33.1 MB\u001b[0m \u001b[31m160.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Downloading huggingface_hub-0.25.2-py3-none-any.whl (436 kB)\n",
      "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
      "Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m178.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (782 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m782.7/782.7 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ruff-0.6.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m217.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading tokenizers-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m150.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typer-0.12.5-py3-none-any.whl (47 kB)\n",
      "Downloading tyro-0.8.11-py3-none-any.whl (105 kB)\n",
      "Downloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "Installing collected packages: sentencepiece, pydub, xxhash, websockets, tomlkit, shtab, shellingham, semantic-version, safetensors, ruff, regex, pyarrow, orjson, fsspec, ffmpy, einops, docstring-parser, dill, av, aiofiles, tiktoken, multiprocess, huggingface-hub, tyro, typer, tokenizers, sse-starlette, gradio-client, transformers, gradio, accelerate, peft, datasets, trl\n",
      "  Attempting uninstall: websockets\n",
      "    Found existing installation: websockets 13.1\n",
      "    Uninstalling websockets-13.1:\n",
      "      Successfully uninstalled websockets-13.1\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.9.0\n",
      "    Uninstalling fsspec-2024.9.0:\n",
      "      Successfully uninstalled fsspec-2024.9.0\n",
      "Successfully installed accelerate-0.34.2 aiofiles-23.2.1 av-13.1.0 datasets-2.21.0 dill-0.3.8 docstring-parser-0.16 einops-0.8.0 ffmpy-0.4.0 fsspec-2024.6.1 gradio-5.0.0 gradio-client-1.4.0 huggingface-hub-0.25.2 multiprocess-0.70.16 orjson-3.10.7 peft-0.12.0 pyarrow-17.0.0 pydub-0.25.1 regex-2024.9.11 ruff-0.6.9 safetensors-0.4.5 semantic-version-2.10.0 sentencepiece-0.2.0 shellingham-1.5.4 shtab-1.7.1 sse-starlette-2.1.3 tiktoken-0.8.0 tokenizers-0.20.0 tomlkit-0.12.0 transformers-4.45.2 trl-0.9.6 typer-0.12.5 tyro-0.8.11 websockets-12.0 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: torch in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from bitsandbytes) (2.2.1+cu121)\n",
      "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->bitsandbytes) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->bitsandbytes) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->bitsandbytes) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->bitsandbytes) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->bitsandbytes) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->bitsandbytes) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->bitsandbytes) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
      "Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m141.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.44.1\n"
     ]
    }
   ],
   "source": [
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers.git\n",
      "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-kgp43lsr\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-kgp43lsr\n",
      "\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit 6ac5f25bb645b61ed2e77f80e39885b4a722e1ba\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (2024.9.11)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (0.20.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.0.dev0) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.0.dev0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers==4.46.0.dev0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers==4.46.0.dev0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers==4.46.0.dev0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers==4.46.0.dev0) (2024.8.30)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.46.0.dev0-py3-none-any.whl size=9962492 sha256=fe5d82550c396f198dc088d0af3c99879b1e8ff7ff95951bc739563c73799913\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-b0z7121z/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\n",
      "Successfully built transformers\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.45.2\n",
      "    Uninstalling transformers-4.45.2:\n",
      "      Successfully uninstalled transformers-4.45.2\n",
      "Successfully installed transformers-4.46.0.dev0\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///teamspace/studios/this_studio/LLaMA-Factory\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers<=4.45.2,>=4.41.2 (from llamafactory==0.9.1.dev0)\n",
      "  Using cached transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: datasets<=2.21.0,>=2.16.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (2.21.0)\n",
      "Requirement already satisfied: accelerate<=0.34.2,>=0.30.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (0.34.2)\n",
      "Requirement already satisfied: peft<=0.12.0,>=0.11.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (0.12.0)\n",
      "Requirement already satisfied: trl<=0.9.6,>=0.8.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (0.9.6)\n",
      "Requirement already satisfied: gradio>=4.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (5.0.0)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (2.1.4)\n",
      "Requirement already satisfied: scipy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (1.11.4)\n",
      "Requirement already satisfied: einops in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (0.8.0)\n",
      "Requirement already satisfied: sentencepiece in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (0.2.0)\n",
      "Requirement already satisfied: tiktoken in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (0.8.0)\n",
      "Requirement already satisfied: protobuf in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (4.23.4)\n",
      "Requirement already satisfied: uvicorn in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (0.31.0)\n",
      "Requirement already satisfied: pydantic in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (2.9.2)\n",
      "Requirement already satisfied: fastapi in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (0.115.0)\n",
      "Requirement already satisfied: sse-starlette in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (2.1.3)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (3.8.2)\n",
      "Requirement already satisfied: fire in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (0.7.0)\n",
      "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (24.1)\n",
      "Requirement already satisfied: pyyaml in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (6.0.2)\n",
      "Requirement already satisfied: numpy<2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (1.26.4)\n",
      "Requirement already satisfied: av in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (13.1.0)\n",
      "Collecting nltk (from llamafactory==0.9.1.dev0)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting jieba (from llamafactory==0.9.1.dev0)\n",
      "  Downloading jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m153.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting rouge-chinese (from llamafactory==0.9.1.dev0)\n",
      "  Downloading rouge_chinese-1.0.3-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: torch>=1.13.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (2.2.1+cu121)\n",
      "Requirement already satisfied: psutil in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate<=0.34.2,>=0.30.1->llamafactory==0.9.1.dev0) (6.0.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate<=0.34.2,>=0.30.1->llamafactory==0.9.1.dev0) (0.25.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate<=0.34.2,>=0.30.1->llamafactory==0.9.1.dev0) (0.4.5)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (3.10.8)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (23.2.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (4.6.0)\n",
      "Requirement already satisfied: ffmpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (0.4.0)\n",
      "Requirement already satisfied: gradio-client==1.4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (1.4.0)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (0.27.2)\n",
      "Requirement already satisfied: jinja2<4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (3.1.4)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (2.1.5)\n",
      "Requirement already satisfied: orjson~=3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (3.10.7)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (10.4.0)\n",
      "Requirement already satisfied: pydub in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (0.0.12)\n",
      "Requirement already satisfied: ruff>=0.2.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (0.6.9)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (0.12.5)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (4.12.2)\n",
      "Requirement already satisfied: websockets<13.0,>=10.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio-client==1.4.0->gradio>=4.0.0->llamafactory==0.9.1.dev0) (12.0)\n",
      "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fastapi->llamafactory==0.9.1.dev0) (0.38.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas>=2.0.0->llamafactory==0.9.1.dev0) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas>=2.0.0->llamafactory==0.9.1.dev0) (2024.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pydantic->llamafactory==0.9.1.dev0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pydantic->llamafactory==0.9.1.dev0) (2.23.4)\n",
      "Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (3.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.1->llamafactory==0.9.1.dev0) (12.6.77)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers<=4.45.2,>=4.41.2->llamafactory==0.9.1.dev0) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers<=4.45.2,>=4.41.2->llamafactory==0.9.1.dev0) (0.20.0)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from trl<=0.9.6,>=0.8.6->llamafactory==0.9.1.dev0) (0.8.11)\n",
      "Requirement already satisfied: click>=7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from uvicorn->llamafactory==0.9.1.dev0) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from uvicorn->llamafactory==0.9.1.dev0) (0.14.0)\n",
      "Requirement already satisfied: termcolor in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fire->llamafactory==0.9.1.dev0) (2.4.0)\n",
      "Requirement already satisfied: joblib in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nltk->llamafactory==0.9.1.dev0) (1.4.2)\n",
      "Requirement already satisfied: six in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rouge-chinese->llamafactory==0.9.1.dev0) (1.16.0)\n",
      "Requirement already satisfied: idna>=2.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->llamafactory==0.9.1.dev0) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->llamafactory==0.9.1.dev0) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->llamafactory==0.9.1.dev0) (1.2.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (1.13.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (4.0.3)\n",
      "Requirement already satisfied: certifi in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.9.1.dev0) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.9.1.dev0) (1.0.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (2.2.3)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.9.1.dev0) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.9.1.dev0) (13.9.1)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tyro>=0.5.11->trl<=0.9.6,>=0.8.6->llamafactory==0.9.1.dev0) (0.16)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tyro>=0.5.11->trl<=0.9.6,>=0.8.6->llamafactory==0.9.1.dev0) (1.7.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch>=1.13.1->llamafactory==0.9.1.dev0) (1.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.9.1.dev0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.9.1.dev0) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.9.1.dev0) (0.1.2)\n",
      "Using cached transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\n",
      "Building wheels for collected packages: llamafactory, jieba\n",
      "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llamafactory: filename=llamafactory-0.9.1.dev0-0.editable-py3-none-any.whl size=22536 sha256=630109962d22315ae8fb9a1111be1bbb8c9b0b6d0fafb56c2487a032fbe09618\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-tv0l58p8/wheels/13/c5/e7/fe27d4add0257cb33dbe5e5426744be2b952735043db6a7cd3\n",
      "  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=ebe4d02dc002cacd35eb8f79efbd5a4cf3897ac2ce2cc94e4c75369bf14f4aab\n",
      "  Stored in directory: /home/zeus/.cache/pip/wheels/c9/69/31/d56d90b22a1777b0b231e234b00302a55be255930f8bd92dcd\n",
      "Successfully built llamafactory jieba\n",
      "Installing collected packages: jieba, rouge-chinese, nltk, transformers, llamafactory\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.46.0.dev0\n",
      "    Uninstalling transformers-4.46.0.dev0:\n",
      "      Successfully uninstalled transformers-4.46.0.dev0\n",
      "Successfully installed jieba-0.42.1 llamafactory-0.9.1.dev0 nltk-3.9.1 rouge-chinese-1.0.3 transformers-4.45.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -e \".[torch, metrics]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting liger-kernel\n",
      "  Downloading liger_kernel-0.3.1-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: torch>=2.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from liger-kernel) (2.2.1+cu121)\n",
      "Collecting triton>=2.3.0 (from liger-kernel)\n",
      "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.1.2->liger-kernel) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.1.2->liger-kernel) (4.12.2)\n",
      "Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.1.2->liger-kernel) (1.13.3)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.1.2->liger-kernel) (3.3)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.1.2->liger-kernel) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.1.2->liger-kernel) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.1.2->liger-kernel) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.1.2->liger-kernel) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.1.2->liger-kernel) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.1.2->liger-kernel) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.1.2->liger-kernel) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.1.2->liger-kernel) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.1.2->liger-kernel) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.1.2->liger-kernel) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.1.2->liger-kernel) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.1.2->liger-kernel) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.1.2->liger-kernel) (12.1.105)\n",
      "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torch>=2.1.2 (from liger-kernel)\n",
      "  Downloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.1.2->liger-kernel)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=2.1.2->liger-kernel)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.1.2->liger-kernel) (12.6.77)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch>=2.1.2->liger-kernel) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch>=2.1.2->liger-kernel) (1.3.0)\n",
      "Downloading liger_kernel-0.3.1-py3-none-any.whl (58 kB)\n",
      "Downloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl (797.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.1/797.1 MB\u001b[0m \u001b[31m188.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m226.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m207.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m319.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-nccl-cu12, nvidia-cudnn-cu12, torch, liger-kernel\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.2.0\n",
      "    Uninstalling triton-2.2.0:\n",
      "      Successfully uninstalled triton-2.2.0\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.19.3\n",
      "    Uninstalling nvidia-nccl-cu12-2.19.3:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.19.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 8.9.2.26\n",
      "    Uninstalling nvidia-cudnn-cu12-8.9.2.26:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-8.9.2.26\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.2.1+cu121\n",
      "    Uninstalling torch-2.2.1+cu121:\n",
      "      Successfully uninstalled torch-2.2.1+cu121\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.17.1+cu121 requires torch==2.2.1, but you have torch 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed liger-kernel-0.3.1 nvidia-cudnn-cu12-9.1.0.70 nvidia-nccl-cu12-2.20.5 torch-2.4.1 triton-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install liger-kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "args = dict(\n",
    "  stage=\"sft\",                       \n",
    "  do_train=True,\n",
    "  model_name_or_path=\"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "  dataset=\"Dermatech_vlm\",             \n",
    "  template=\"qwen2_vl\",                     # use llama3 prompt template\n",
    "  finetuning_type=\"lora\",                   # use LoRA adapters to save memory\n",
    "  lora_target=\"all\",                     # attach LoRA adapters to all linear layers\n",
    "  output_dir=\"qwen2vl_lora\",                  # the path to save LoRA adapters\n",
    "  per_device_train_batch_size=2,               # the batch size\n",
    "  gradient_accumulation_steps=4,               # the gradient accumulation steps\n",
    "  lr_scheduler_type=\"cosine\",                 # use cosine learning rate scheduler\n",
    "  logging_steps=10,                      # log every 10 steps\n",
    "  warmup_ratio=0.1,                      # use warmup scheduler\n",
    "  save_steps=1000,                      # save checkpoint every 1000 steps\n",
    "  learning_rate=4e-4,                     # the learning rate\n",
    "  num_train_epochs=3.3,                    # the epochs of training\n",
    "  max_samples=1500,                      \n",
    "  max_grad_norm=1.0,                     # clip gradient norm to 1.0\n",
    "  loraplus_lr_ratio=16.0,                   # use LoRA+ algorithm with lambda=16.0\n",
    "  bf16=True,                         # use truncated FP16 mixed precision training\n",
    "  use_liger_kernel=True,                   # use liger kernel for efficient training\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(args, open(\"train_qwen2vl.json\", \"w\", encoding=\"utf-8\"), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/LLaMA-Factory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd LLaMA-Factory/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/teamspace/studios/this_studio/LLaMA-Factory'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10/2024 08:55:37 - INFO - llamafactory.hparams.parser - Resuming training from qwen2vl_lora/checkpoint-205.\n",
      "10/10/2024 08:55:37 - INFO - llamafactory.hparams.parser - Change `output_dir` or use `overwrite_output_dir` to avoid.\n",
      "10/10/2024 08:55:37 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.bfloat16\n",
      "[INFO|configuration_utils.py:675] 2024-10-10 08:55:37,712 >> loading configuration file config.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/config.json\n",
      "[INFO|configuration_utils.py:742] 2024-10-10 08:55:37,714 >> Model config Qwen2VLConfig {\n",
      "  \"_name_or_path\": \"Qwen/Qwen2-VL-2B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2VLForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"image_token_id\": 151655,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2_vl\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"mrope_section\": [\n",
      "      16,\n",
      "      24,\n",
      "      24\n",
      "    ],\n",
      "    \"rope_type\": \"default\",\n",
      "    \"type\": \"default\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"video_token_id\": 151656,\n",
      "  \"vision_config\": {\n",
      "    \"hidden_size\": 1536,\n",
      "    \"in_chans\": 3,\n",
      "    \"model_type\": \"qwen2_vl\",\n",
      "    \"spatial_patch_size\": 14\n",
      "  },\n",
      "  \"vision_end_token_id\": 151653,\n",
      "  \"vision_start_token_id\": 151652,\n",
      "  \"vision_token_id\": 151654,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-10-10 08:55:37,747 >> loading file vocab.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-10-10 08:55:37,747 >> loading file merges.txt from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-10-10 08:55:37,747 >> loading file tokenizer.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-10-10 08:55:37,747 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-10-10 08:55:37,747 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-10-10 08:55:37,747 >> loading file tokenizer_config.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2470] 2024-10-10 08:55:37,984 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|image_processing_base.py:375] 2024-10-10 08:55:38,039 >> loading configuration file preprocessor_config.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/preprocessor_config.json\n",
      "[INFO|image_processing_base.py:375] 2024-10-10 08:55:38,064 >> loading configuration file preprocessor_config.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/preprocessor_config.json\n",
      "[INFO|image_processing_base.py:429] 2024-10-10 08:55:38,064 >> Image processor Qwen2VLImageProcessor {\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"Qwen2VLImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"max_pixels\": 12845056,\n",
      "  \"merge_size\": 2,\n",
      "  \"min_pixels\": 3136,\n",
      "  \"patch_size\": 14,\n",
      "  \"processor_class\": \"Qwen2VLProcessor\",\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"max_pixels\": 12845056,\n",
      "    \"min_pixels\": 3136\n",
      "  },\n",
      "  \"temporal_patch_size\": 2\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-10-10 08:55:38,252 >> loading file vocab.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-10-10 08:55:38,252 >> loading file merges.txt from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-10-10 08:55:38,252 >> loading file tokenizer.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-10-10 08:55:38,252 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-10-10 08:55:38,252 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-10-10 08:55:38,252 >> loading file tokenizer_config.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2470] 2024-10-10 08:55:38,480 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|processing_utils.py:744] 2024-10-10 08:55:38,893 >> Processor Qwen2VLProcessor:\n",
      "- image_processor: Qwen2VLImageProcessor {\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"Qwen2VLImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"max_pixels\": 12845056,\n",
      "  \"merge_size\": 2,\n",
      "  \"min_pixels\": 3136,\n",
      "  \"patch_size\": 14,\n",
      "  \"processor_class\": \"Qwen2VLProcessor\",\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"max_pixels\": 12845056,\n",
      "    \"min_pixels\": 3136\n",
      "  },\n",
      "  \"temporal_patch_size\": 2\n",
      "}\n",
      "\n",
      "- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2-VL-2B-Instruct', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "\n",
      "{\n",
      "  \"processor_class\": \"Qwen2VLProcessor\"\n",
      "}\n",
      "\n",
      "10/10/2024 08:55:38 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "10/10/2024 08:55:38 - INFO - llamafactory.data.loader - Loading dataset Dermatech_vlm.json...\n",
      "Converting format of dataset: 100%|█| 1500/1500 [00:00<00:00, 11533.06 examples/\n",
      "Running tokenizer on dataset: 100%|██| 1500/1500 [00:22<00:00, 65.33 examples/s]\n",
      "training example:\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 40, 1079, 12880, 419, 6787, 2971, 13, 151645, 198, 151644, 77091, 198, 48113, 22308, 13876, 28660, 525, 3042, 389, 279, 13309, 315, 279, 4478, 11, 892, 1231, 13216, 264, 66912, 509, 69404, 834, 85, 355, 476, 66912, 7786, 13, 1084, 374, 11102, 311, 36671, 88861, 476, 60385, 436, 8560, 23568, 369, 264, 803, 44713, 22982, 13, 576, 2971, 4977, 311, 387, 66912, 7786, 3419, 1331, 32362, 13, 151645]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "<|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|>I am facing this skin condition.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Irregular brown patches are present on the sole of the foot, which may indicate a melanocytic nevus or melanoma. It is recommended to undergo pathological or dermatoscopy examination for a more definitive diagnosis. The condition seems to be melanoma-in-situ.<|im_end|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 48113, 22308, 13876, 28660, 525, 3042, 389, 279, 13309, 315, 279, 4478, 11, 892, 1231, 13216, 264, 66912, 509, 69404, 834, 85, 355, 476, 66912, 7786, 13, 1084, 374, 11102, 311, 36671, 88861, 476, 60385, 436, 8560, 23568, 369, 264, 803, 44713, 22982, 13, 576, 2971, 4977, 311, 387, 66912, 7786, 3419, 1331, 32362, 13, 151645]\n",
      "labels:\n",
      "Irregular brown patches are present on the sole of the foot, which may indicate a melanocytic nevus or melanoma. It is recommended to undergo pathological or dermatoscopy examination for a more definitive diagnosis. The condition seems to be melanoma-in-situ.<|im_end|>\n",
      "[INFO|configuration_utils.py:675] 2024-10-10 08:56:02,150 >> loading configuration file config.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/config.json\n",
      "[INFO|configuration_utils.py:742] 2024-10-10 08:56:02,152 >> Model config Qwen2VLConfig {\n",
      "  \"_name_or_path\": \"Qwen/Qwen2-VL-2B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2VLForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"image_token_id\": 151655,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2_vl\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"mrope_section\": [\n",
      "      16,\n",
      "      24,\n",
      "      24\n",
      "    ],\n",
      "    \"rope_type\": \"default\",\n",
      "    \"type\": \"default\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"video_token_id\": 151656,\n",
      "  \"vision_config\": {\n",
      "    \"hidden_size\": 1536,\n",
      "    \"in_chans\": 3,\n",
      "    \"model_type\": \"qwen2_vl\",\n",
      "    \"spatial_patch_size\": 14\n",
      "  },\n",
      "  \"vision_end_token_id\": 151653,\n",
      "  \"vision_start_token_id\": 151652,\n",
      "  \"vision_token_id\": 151654,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3732] 2024-10-10 08:56:02,165 >> loading weights file model.safetensors from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1622] 2024-10-10 08:56:02,166 >> Instantiating Qwen2VLForConditionalGeneration model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1099] 2024-10-10 08:56:02,167 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[WARNING|logging.py:328] 2024-10-10 08:56:02,191 >> `Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.54it/s]\n",
      "[INFO|modeling_utils.py:4574] 2024-10-10 08:56:03,579 >> All model checkpoint weights were used when initializing Qwen2VLForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:4582] 2024-10-10 08:56:03,579 >> All the weights of Qwen2VLForConditionalGeneration were initialized from the model checkpoint at Qwen/Qwen2-VL-2B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2VLForConditionalGeneration for predictions without further training.\n",
      "[INFO|configuration_utils.py:1054] 2024-10-10 08:56:03,601 >> loading configuration file generation_config.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/generation_config.json\n",
      "[INFO|configuration_utils.py:1099] 2024-10-10 08:56:03,602 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"temperature\": 0.01,\n",
      "  \"top_k\": 1,\n",
      "  \"top_p\": 0.001\n",
      "}\n",
      "\n",
      "10/10/2024 08:56:03 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "10/10/2024 08:56:03 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "10/10/2024 08:56:03 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "10/10/2024 08:56:03 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "10/10/2024 08:56:03 - INFO - llamafactory.model.model_utils.misc - Found linear modules: v_proj,up_proj,gate_proj,k_proj,o_proj,q_proj,down_proj\n",
      "10/10/2024 08:56:03 - INFO - llamafactory.model.loader - trainable params: 9,232,384 || all params: 2,218,217,984 || trainable%: 0.4162\n",
      "[WARNING|trainer.py:477] 2024-10-10 08:56:03,979 >> The model is not an instance of PreTrainedModel. No liger kernels will be applied.\n",
      "[INFO|trainer.py:667] 2024-10-10 08:56:04,038 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2637] 2024-10-10 08:56:04,039 >> Loading model from qwen2vl_lora/checkpoint-205.\n",
      "10/10/2024 08:56:04 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/trainer.py:3262: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "[INFO|trainer.py:2243] 2024-10-10 08:56:04,660 >> ***** Running training *****\n",
      "[INFO|trainer.py:2244] 2024-10-10 08:56:04,660 >>   Num examples = 1,500\n",
      "[INFO|trainer.py:2245] 2024-10-10 08:56:04,660 >>   Num Epochs = 4\n",
      "[INFO|trainer.py:2246] 2024-10-10 08:56:04,660 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:2249] 2024-10-10 08:56:04,661 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:2250] 2024-10-10 08:56:04,661 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:2251] 2024-10-10 08:56:04,661 >>   Total optimization steps = 618\n",
      "[INFO|trainer.py:2252] 2024-10-10 08:56:04,666 >>   Number of trainable parameters = 9,232,384\n",
      "[INFO|trainer.py:2274] 2024-10-10 08:56:04,666 >>   Continuing training from checkpoint, will skip to saved global_step\n",
      "[INFO|trainer.py:2275] 2024-10-10 08:56:04,666 >>   Continuing training from epoch 1\n",
      "[INFO|trainer.py:2276] 2024-10-10 08:56:04,666 >>   Continuing training from global step 205\n",
      "[INFO|trainer.py:2278] 2024-10-10 08:56:04,666 >>   Will skip the first 1 epochs then the first 72 batches in the first epoch.\n",
      "  0%|                                                   | 0/618 [00:00<?, ?it/s]/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/trainer.py:2944: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 1.7186, 'grad_norm': 4.197771072387695, 'learning_rate': 0.0003340499173135062, 'epoch': 1.12}\n",
      "{'loss': 1.1239, 'grad_norm': 1.976620078086853, 'learning_rate': 0.0003254538010240608, 'epoch': 1.18}\n",
      "{'loss': 1.2483, 'grad_norm': 1.6347882747650146, 'learning_rate': 0.0003164572622668787, 'epoch': 1.23}\n",
      "{'loss': 1.1674, 'grad_norm': 1.7263752222061157, 'learning_rate': 0.00030708901612449104, 'epoch': 1.28}\n",
      "{'loss': 1.2562, 'grad_norm': 1.6630886793136597, 'learning_rate': 0.000297378964092174, 'epoch': 1.34}\n",
      "{'loss': 1.1307, 'grad_norm': 1.7403186559677124, 'learning_rate': 0.00028735809863858844, 'epoch': 1.39}\n",
      "{'loss': 1.1103, 'grad_norm': 1.7649906873703003, 'learning_rate': 0.0002770584042842593, 'epoch': 1.44}\n",
      "{'loss': 1.1205, 'grad_norm': 2.424694061279297, 'learning_rate': 0.0002665127555136312, 'epoch': 1.5}\n",
      "{'loss': 1.1252, 'grad_norm': 1.934572696685791, 'learning_rate': 0.0002557548118465446, 'epoch': 1.55}\n",
      "{'loss': 1.1119, 'grad_norm': 1.5292526483535767, 'learning_rate': 0.00024481891040404154, 'epoch': 1.6}\n",
      "{'loss': 1.0153, 'grad_norm': 2.4333550930023193, 'learning_rate': 0.00023373995631140946, 'epoch': 1.66}\n",
      "{'loss': 1.0508, 'grad_norm': 1.7339928150177002, 'learning_rate': 0.00022255331128827338, 'epoch': 1.71}\n",
      "{'loss': 1.0544, 'grad_norm': 1.7087345123291016, 'learning_rate': 0.00021129468078133398, 'epoch': 1.76}\n",
      "{'loss': 1.0261, 'grad_norm': 1.6827566623687744, 'learning_rate': 0.0002, 'epoch': 1.82}\n",
      "{'loss': 1.0521, 'grad_norm': 1.7459943294525146, 'learning_rate': 0.00018870531921866604, 'epoch': 1.87}\n",
      "{'loss': 1.0911, 'grad_norm': 2.020001173019409, 'learning_rate': 0.00017744668871172664, 'epoch': 1.92}\n",
      "{'loss': 1.0321, 'grad_norm': 1.523681640625, 'learning_rate': 0.00016626004368859058, 'epoch': 1.98}\n",
      "{'loss': 0.8083, 'grad_norm': 1.4960293769836426, 'learning_rate': 0.00015518108959595848, 'epoch': 2.03}\n",
      "{'loss': 0.741, 'grad_norm': 1.9710869789123535, 'learning_rate': 0.0001442451881534554, 'epoch': 2.08}\n",
      "{'loss': 0.6804, 'grad_norm': 1.5198588371276855, 'learning_rate': 0.0001334872444863688, 'epoch': 2.14}\n",
      "{'loss': 0.7061, 'grad_norm': 1.5888607501983643, 'learning_rate': 0.00012294159571574076, 'epoch': 2.19}\n",
      "{'loss': 0.6881, 'grad_norm': 1.6064332723617554, 'learning_rate': 0.00011264190136141164, 'epoch': 2.24}\n",
      "{'loss': 0.7007, 'grad_norm': 1.9456806182861328, 'learning_rate': 0.00010262103590782603, 'epoch': 2.3}\n",
      "{'loss': 0.6162, 'grad_norm': 1.959385633468628, 'learning_rate': 9.291098387550902e-05, 'epoch': 2.35}\n",
      "{'loss': 0.6605, 'grad_norm': 1.6649196147918701, 'learning_rate': 8.354273773312133e-05, 'epoch': 2.4}\n",
      "{'loss': 0.6804, 'grad_norm': 2.0848257541656494, 'learning_rate': 7.454619897593927e-05, 'epoch': 2.46}\n",
      "{'loss': 0.7001, 'grad_norm': 1.9497970342636108, 'learning_rate': 6.595008268649387e-05, 'epoch': 2.51}\n",
      "{'loss': 0.6938, 'grad_norm': 1.5712602138519287, 'learning_rate': 5.7781825881993566e-05, 'epoch': 2.56}\n",
      "{'loss': 0.6783, 'grad_norm': 1.841971516609192, 'learning_rate': 5.006749994106603e-05, 'epoch': 2.62}\n",
      "{'loss': 0.6564, 'grad_norm': 1.8594976663589478, 'learning_rate': 4.283172738933394e-05, 'epoch': 2.67}\n",
      "{'loss': 0.6523, 'grad_norm': 1.6053839921951294, 'learning_rate': 3.6097603309429375e-05, 'epoch': 2.72}\n",
      "{'loss': 0.6534, 'grad_norm': 1.7930959463119507, 'learning_rate': 2.9886621626288013e-05, 'epoch': 2.78}\n",
      "{'loss': 0.6448, 'grad_norm': 1.69853675365448, 'learning_rate': 2.421860650300687e-05, 'epoch': 2.83}\n",
      "{'loss': 0.6195, 'grad_norm': 1.9236022233963013, 'learning_rate': 1.9111649066234127e-05, 'epoch': 2.88}\n",
      "{'loss': 0.5821, 'grad_norm': 1.7498300075531006, 'learning_rate': 1.4582049663051922e-05, 'epoch': 2.94}\n",
      "{'loss': 0.6799, 'grad_norm': 1.5894365310668945, 'learning_rate': 1.0644265833654965e-05, 'epoch': 2.99}\n",
      "{'loss': 0.4822, 'grad_norm': 1.2726022005081177, 'learning_rate': 7.310866165885944e-06, 'epoch': 3.04}\n",
      "{'loss': 0.4512, 'grad_norm': 1.682220697402954, 'learning_rate': 4.592490178914721e-06, 'epoch': 3.1}\n",
      "{'loss': 0.4173, 'grad_norm': 1.3848233222961426, 'learning_rate': 2.497814364103479e-06, 'epoch': 3.15}\n",
      "{'loss': 0.4073, 'grad_norm': 1.2645756006240845, 'learning_rate': 1.033524491449045e-06, 'epoch': 3.2}\n",
      "{'loss': 0.3486, 'grad_norm': 1.1448299884796143, 'learning_rate': 2.0429426999413193e-07, 'epoch': 3.26}\n",
      "100%|█████████████████████████████████████████| 618/618 [09:01<00:00,  1.31s/it][INFO|trainer.py:3705] 2024-10-10 09:05:06,188 >> Saving model checkpoint to qwen2vl_lora/checkpoint-618\n",
      "[INFO|configuration_utils.py:675] 2024-10-10 09:05:06,423 >> loading configuration file config.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/config.json\n",
      "[INFO|configuration_utils.py:742] 2024-10-10 09:05:06,424 >> Model config Qwen2VLConfig {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2VLForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"image_token_id\": 151655,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2_vl\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"mrope_section\": [\n",
      "      16,\n",
      "      24,\n",
      "      24\n",
      "    ],\n",
      "    \"rope_type\": \"default\",\n",
      "    \"type\": \"default\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"video_token_id\": 151656,\n",
      "  \"vision_config\": {\n",
      "    \"hidden_size\": 1536,\n",
      "    \"in_chans\": 3,\n",
      "    \"model_type\": \"qwen2_vl\",\n",
      "    \"spatial_patch_size\": 14\n",
      "  },\n",
      "  \"vision_end_token_id\": 151653,\n",
      "  \"vision_start_token_id\": 151652,\n",
      "  \"vision_token_id\": 151654,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2641] 2024-10-10 09:05:06,499 >> tokenizer config file saved in qwen2vl_lora/checkpoint-618/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2650] 2024-10-10 09:05:06,500 >> Special tokens file saved in qwen2vl_lora/checkpoint-618/special_tokens_map.json\n",
      "[INFO|image_processing_base.py:258] 2024-10-10 09:05:06,842 >> Image processor saved in qwen2vl_lora/checkpoint-618/preprocessor_config.json\n",
      "[INFO|trainer.py:2505] 2024-10-10 09:05:06,842 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 542.1764, 'train_samples_per_second': 9.13, 'train_steps_per_second': 1.14, 'train_loss': 0.5477117018406445, 'epoch': 3.3}\n",
      "100%|█████████████████████████████████████████| 618/618 [09:02<00:00,  1.14it/s]\n",
      "[INFO|image_processing_base.py:258] 2024-10-10 09:05:06,845 >> Image processor saved in qwen2vl_lora/preprocessor_config.json\n",
      "[INFO|trainer.py:3705] 2024-10-10 09:05:06,845 >> Saving model checkpoint to qwen2vl_lora\n",
      "[INFO|configuration_utils.py:675] 2024-10-10 09:05:06,916 >> loading configuration file config.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/config.json\n",
      "[INFO|configuration_utils.py:742] 2024-10-10 09:05:06,917 >> Model config Qwen2VLConfig {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2VLForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"image_token_id\": 151655,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2_vl\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"mrope_section\": [\n",
      "      16,\n",
      "      24,\n",
      "      24\n",
      "    ],\n",
      "    \"rope_type\": \"default\",\n",
      "    \"type\": \"default\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"video_token_id\": 151656,\n",
      "  \"vision_config\": {\n",
      "    \"hidden_size\": 1536,\n",
      "    \"in_chans\": 3,\n",
      "    \"model_type\": \"qwen2_vl\",\n",
      "    \"spatial_patch_size\": 14\n",
      "  },\n",
      "  \"vision_end_token_id\": 151653,\n",
      "  \"vision_start_token_id\": 151652,\n",
      "  \"vision_token_id\": 151654,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2641] 2024-10-10 09:05:07,014 >> tokenizer config file saved in qwen2vl_lora/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2650] 2024-10-10 09:05:07,014 >> Special tokens file saved in qwen2vl_lora/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =     3.2987\n",
      "  total_flos               = 20309817GF\n",
      "  train_loss               =     0.5477\n",
      "  train_runtime            = 0:09:02.17\n",
      "  train_samples_per_second =       9.13\n",
      "  train_steps_per_second   =       1.14\n",
      "[INFO|modelcard.py:449] 2024-10-10 09:05:07,155 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!llamafactory-cli train train_qwen2vl.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dict(\n",
    "  model_name_or_path=\"Qwen/Qwen2-VL-2B-Instruct\", # use official non-quantized Llama-3-8B-Instruct model\n",
    "  adapter_name_or_path=\"qwen2vl_lora\",            # load the saved LoRA adapters\n",
    "  template=\"qwen2_vl\",                     # same to the one in training\n",
    "  finetuning_type=\"lora\",                  # same to the one in training\n",
    "  export_dir=\"qwen2vl_2b_instruct_lora_merged\",              # the path to save the merged model\n",
    "  export_size=2,                       # the file shard size (in GB) of the merged model\n",
    "  export_device=\"cpu\",                    # the device used in export, can be chosen from `cpu` and `cuda`\n",
    "  #export_hub_model_id=\"your_id/your_model\",         # the Hugging Face hub ID to upload model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/LLaMA-Factory\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "%cd /teamspace/studios/this_studio/LLaMA-Factory\n",
    "json.dump(args, open(\"merge_qwen2vl.json\", \"w\", encoding=\"utf-8\"), indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:675] 2024-10-10 09:11:01,292 >> loading configuration file config.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/config.json\n",
      "[INFO|configuration_utils.py:742] 2024-10-10 09:11:01,294 >> Model config Qwen2VLConfig {\n",
      "  \"_name_or_path\": \"Qwen/Qwen2-VL-2B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2VLForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"image_token_id\": 151655,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2_vl\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"mrope_section\": [\n",
      "      16,\n",
      "      24,\n",
      "      24\n",
      "    ],\n",
      "    \"rope_type\": \"default\",\n",
      "    \"type\": \"default\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"video_token_id\": 151656,\n",
      "  \"vision_config\": {\n",
      "    \"hidden_size\": 1536,\n",
      "    \"in_chans\": 3,\n",
      "    \"model_type\": \"qwen2_vl\",\n",
      "    \"spatial_patch_size\": 14\n",
      "  },\n",
      "  \"vision_end_token_id\": 151653,\n",
      "  \"vision_start_token_id\": 151652,\n",
      "  \"vision_token_id\": 151654,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-10-10 09:11:01,318 >> loading file vocab.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-10-10 09:11:01,318 >> loading file merges.txt from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-10-10 09:11:01,318 >> loading file tokenizer.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-10-10 09:11:01,318 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-10-10 09:11:01,318 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-10-10 09:11:01,318 >> loading file tokenizer_config.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2470] 2024-10-10 09:11:01,616 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|image_processing_base.py:375] 2024-10-10 09:11:01,681 >> loading configuration file preprocessor_config.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/preprocessor_config.json\n",
      "[INFO|image_processing_base.py:375] 2024-10-10 09:11:01,701 >> loading configuration file preprocessor_config.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/preprocessor_config.json\n",
      "[INFO|image_processing_base.py:429] 2024-10-10 09:11:01,701 >> Image processor Qwen2VLImageProcessor {\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"Qwen2VLImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"max_pixels\": 12845056,\n",
      "  \"merge_size\": 2,\n",
      "  \"min_pixels\": 3136,\n",
      "  \"patch_size\": 14,\n",
      "  \"processor_class\": \"Qwen2VLProcessor\",\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"max_pixels\": 12845056,\n",
      "    \"min_pixels\": 3136\n",
      "  },\n",
      "  \"temporal_patch_size\": 2\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-10-10 09:11:01,723 >> loading file vocab.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-10-10 09:11:01,723 >> loading file merges.txt from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-10-10 09:11:01,723 >> loading file tokenizer.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-10-10 09:11:01,723 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-10-10 09:11:01,723 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-10-10 09:11:01,723 >> loading file tokenizer_config.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2470] 2024-10-10 09:11:02,054 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|processing_utils.py:744] 2024-10-10 09:11:02,580 >> Processor Qwen2VLProcessor:\n",
      "- image_processor: Qwen2VLImageProcessor {\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"Qwen2VLImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"max_pixels\": 12845056,\n",
      "  \"merge_size\": 2,\n",
      "  \"min_pixels\": 3136,\n",
      "  \"patch_size\": 14,\n",
      "  \"processor_class\": \"Qwen2VLProcessor\",\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"max_pixels\": 12845056,\n",
      "    \"min_pixels\": 3136\n",
      "  },\n",
      "  \"temporal_patch_size\": 2\n",
      "}\n",
      "\n",
      "- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2-VL-2B-Instruct', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "\n",
      "{\n",
      "  \"processor_class\": \"Qwen2VLProcessor\"\n",
      "}\n",
      "\n",
      "10/10/2024 09:11:02 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "[INFO|configuration_utils.py:675] 2024-10-10 09:11:02,631 >> loading configuration file config.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/config.json\n",
      "[INFO|configuration_utils.py:742] 2024-10-10 09:11:02,632 >> Model config Qwen2VLConfig {\n",
      "  \"_name_or_path\": \"Qwen/Qwen2-VL-2B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2VLForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"image_token_id\": 151655,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2_vl\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"mrope_section\": [\n",
      "      16,\n",
      "      24,\n",
      "      24\n",
      "    ],\n",
      "    \"rope_type\": \"default\",\n",
      "    \"type\": \"default\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"video_token_id\": 151656,\n",
      "  \"vision_config\": {\n",
      "    \"hidden_size\": 1536,\n",
      "    \"in_chans\": 3,\n",
      "    \"model_type\": \"qwen2_vl\",\n",
      "    \"spatial_patch_size\": 14\n",
      "  },\n",
      "  \"vision_end_token_id\": 151653,\n",
      "  \"vision_start_token_id\": 151652,\n",
      "  \"vision_token_id\": 151654,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "10/10/2024 09:11:02 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n",
      "[INFO|modeling_utils.py:3732] 2024-10-10 09:11:02,643 >> loading weights file model.safetensors from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1622] 2024-10-10 09:11:02,646 >> Instantiating Qwen2VLForConditionalGeneration model under default dtype torch.float32.\n",
      "[INFO|configuration_utils.py:1099] 2024-10-10 09:11:02,647 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[WARNING|logging.py:328] 2024-10-10 09:11:02,660 >> `Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:08<00:00,  4.26s/it]\n",
      "[INFO|modeling_utils.py:4574] 2024-10-10 09:11:11,288 >> All model checkpoint weights were used when initializing Qwen2VLForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:4582] 2024-10-10 09:11:11,289 >> All the weights of Qwen2VLForConditionalGeneration were initialized from the model checkpoint at Qwen/Qwen2-VL-2B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2VLForConditionalGeneration for predictions without further training.\n",
      "[INFO|configuration_utils.py:1054] 2024-10-10 09:11:11,319 >> loading configuration file generation_config.json from cache at /home/zeus/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/generation_config.json\n",
      "[INFO|configuration_utils.py:1099] 2024-10-10 09:11:11,319 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"temperature\": 0.01,\n",
      "  \"top_k\": 1,\n",
      "  \"top_p\": 0.001\n",
      "}\n",
      "\n",
      "10/10/2024 09:11:11 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "10/10/2024 09:11:14 - INFO - llamafactory.model.adapter - Merged 1 adapter(s).\n",
      "10/10/2024 09:11:14 - INFO - llamafactory.model.adapter - Loaded adapter(s): qwen2vl_lora\n",
      "10/10/2024 09:11:14 - INFO - llamafactory.model.loader - all params: 2,208,985,600\n",
      "10/10/2024 09:11:16 - INFO - llamafactory.train.tuner - Convert model dtype to: torch.bfloat16.\n",
      "[INFO|configuration_utils.py:410] 2024-10-10 09:11:16,545 >> Configuration saved in qwen2vl_2b_instruct_lora_merged/config.json\n",
      "[INFO|configuration_utils.py:868] 2024-10-10 09:11:16,546 >> Configuration saved in qwen2vl_2b_instruct_lora_merged/generation_config.json\n",
      "[INFO|modeling_utils.py:2844] 2024-10-10 09:11:21,327 >> The model is bigger than the maximum size per checkpoint (2GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at qwen2vl_2b_instruct_lora_merged/model.safetensors.index.json.\n",
      "[INFO|tokenization_utils_base.py:2641] 2024-10-10 09:11:21,331 >> tokenizer config file saved in qwen2vl_2b_instruct_lora_merged/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2650] 2024-10-10 09:11:21,331 >> Special tokens file saved in qwen2vl_2b_instruct_lora_merged/special_tokens_map.json\n",
      "[INFO|image_processing_base.py:258] 2024-10-10 09:11:21,560 >> Image processor saved in qwen2vl_2b_instruct_lora_merged/preprocessor_config.json\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli export merge_qwen2vl.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_model_path = \"/teamspace/studios/this_studio/LLaMA-Factory/qwen2vl_2b_instruct_lora_merged\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_repo = \"Rewatiramans/Dermatech-Qwen2-VL-2B\"\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14571778676f4938ac3decaa23b816ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, HfFolder, Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = HfApi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ec618238104f76b6045dac6b439672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/1.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b479f0a859c44b9e9088691a99601076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "542eb5e64b904df69df88977e849f15b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/429M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce804d100124b288f4b5afa57dfe29d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfe94bb312ee4f5c8db4e101c04f1364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Rewatiramans/Dermatech-Qwen2-VL-2B/commit/b7c4df169fa277b27a8ef299cc24bd1cdf415ae5', commit_message='Initial model upload', commit_description='', oid='b7c4df169fa277b27a8ef299cc24bd1cdf415ae5', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Rewatiramans/Dermatech-Qwen2-VL-2B', endpoint='https://huggingface.co', repo_type='model', repo_id='Rewatiramans/Dermatech-Qwen2-VL-2B'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.upload_folder(\n",
    "    folder_path=final_model_path,    # The folder containing the model files\n",
    "    repo_id=hf_model_repo,                # Your authentication token\n",
    "    commit_message=\"Initial model upload\"  # Optional commit message\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Model pushed hehe: {hf_model_repo}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
